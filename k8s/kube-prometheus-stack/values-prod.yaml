defaultRules:
  rules:
    kubeApiserver: true
    kubeApiserverError: true
    kubePrometheusNodeAlerting: true
    kubernetesAbsent: true
    time: true

alertmanager:
  config:
    route:
      group_by: ['job']
      receiver: 'null'
      routes:
        - matchers:
          - severity=critical
          - team!=dev
          receiver: infra_critical_alerts
        - matchers:
          - team!=business
          receiver: infra_non_critical_alerts
     
    receivers:
      - name: slack
        slackConfigs:
          - sendResolved: true
            apiURL:
              name: slack-webhook-url-dev
              key: webhook-url
            channel: '#alerts'
            message: '{{ template "telegram.urbox" . }}'

      - name: slack
        slackConfigs:
          - sendResolved: true
            apiURL:
              name: slack-webhook-url-business
              key: webhook-url
            channel: '#alerts'
            message: '{{ template "telegram.urbox" . }}'

                 
  templateFiles:
    telegram.default.html.tmpl: |-
      {{ range $i, $alert :=.Alerts }}
      {{if eq .Status "firing"}}ðŸ”¥ <b>Status:</b> {{.Status | toUpper }} ðŸ”¥{{else }}âœ… <b>Status:</b> {{.Status | toUpper }} âœ…{{ end }}
      <b>Alert Name:</b> {{ $alert.Labels.alertname }}
      <b>SEVERITY:</b> {{ $alert.Labels.severity | toUpper }} 
      <b>SUMMERY:</b>{{ $alert.Annotations.summary }}
      <b>STARTEDAT:</b>{{ $alert.StartsAt }}
      {{end -}}
      {{- end -}}

prometheus:
  prometheusSpec:
    ruleSelectorNilUsesHelmValues: false
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    storageSpec: 
      volumeClaimTemplate:
        spec:
          storageClassName: openebs-zfspv
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi

grafana:
  ## In order to persist all dashboards, we provide dashboards json files in grafana/dashboards/
  ## directory. then we need to specify a dashboardProviders config file first!
  ## Then we point to the dashboards' files path in the grafana.dashboards.<provider-name> section.
  dashboardProviders:
   dashboardproviders.yaml:
     apiVersion: 1
     providers:
     - name: 'default'
       orgId: 1
       folder: ''
       type: file
       disableDeletion: false
       editable: true
       options:
         path: /var/lib/grafana/dashboards/default
  dashboards: 
    default:
      redis:
        file: dashboards/redis.json
        datasource: Prometheus
      argo:
        file: dashboards/argo.json
        datasource: Prometheus
      elasticsearch:
        file: dashboards/elasticsearch.json
        datasource: Prometheus
  adminPassword: 123456
  grafana.ini:
    server:
      root_url: https://grafana.test.com/

  additionalDataSources:
    - name: Loki
      type: loki
      uid: loki
      access: proxy
      url: http://loki.logging.svc.cluster.local:3100
      jsonData:
        derivedFields:
          - name: trace_id
            datasourceUid: tempo
            matcherRegex: "trace_id=(\\w+)"
            url: '$${__value.raw}'
  ingress:
    enabled: true
    ingressClassName: nginx
    path: /
    hosts:
      - grafana.test.com
    annotations:
      nginx.ingress.kubernetes.io/force-ssl-redirect: "true"

prometheusOperator:
  enabled: true

additionalPrometheusRulesMap:
 custom-rules:
  groups:
  - name: GroupA
    rules:
     - alert: Kubernetes Job Failed 
       expr: kube_job_status_failed > 0
       for: 0m
       labels:
         severity: warning
       annotations:
         summary: Kubernetes Job failed (instance {{ $labels.instance }})
         description: "Job {{$labels.namespace}}/{{$labels.exported_job}} failed to complete\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}" 
     
     - alert: KubernetesVolumeOutOfDiskSpace
       expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 30
       for: 2m
       labels:
         severity: critical
       annotations:
         summary: Kubernetes Volume out of disk space (persistentvolumeclaim {{ $labels.persistentvolumeclaim }})
         description: "Volume is almost full (< 70% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"    
     
     - alert: KubernetesAlmostVolumeOutOfDiskSpace
       expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 50
       for: 2m
       labels:
         severity: warning
       annotations:
         summary: Volume is almost full (persistentvolumeclaim {{ $labels.persistentvolumeclaim }})
         description: "Volume is almost full (< 50% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"    